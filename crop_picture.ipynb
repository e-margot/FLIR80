{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/e-margot/FLIR80/blob/main/crop_picture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H8ZrwcxmdWZ"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "from torchvision.transforms import ToTensor, PILToTensor, ConvertImageDtype, Compose\n",
        "import os.path\n",
        "from typing import Any, Callable, Optional, Tuple, List\n",
        "from torchvision.datasets.vision import VisionDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor, PILToTensor\n",
        "import torchvision.transforms as tt\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "cBIjeobYg0Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-ya92N6u3b-",
        "outputId": "45e1bbbf-b304-4edd-9f0b-924007af62d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crop picture example "
      ],
      "metadata": {
        "id": "lQUBXJjztVC9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LKP_ZeDu1rV"
      },
      "outputs": [],
      "source": [
        "img = Image.open(\"/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_train/data/video-4WXE26tzNxT688hT3-frame-002055-7Ghf3TZLA3GX5fWeT.jpg\")\n",
        "width, height = img.size\n",
        "print(width, height)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oArgFt_WwQLm"
      },
      "outputs": [],
      "source": [
        "display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shGfGccBvjzE"
      },
      "outputs": [],
      "source": [
        "x_left = 147\n",
        "y_left = 499\n",
        "x_right = x_left + 1652\n",
        "y_right = y_left + 285\n",
        "\n",
        "area = (x_left, y_left, x_right, y_right)\n",
        "cropped_img = img.crop(area)\n",
        "cropped_img.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73BDubW13Zkf"
      },
      "outputs": [],
      "source": [
        "display(cropped_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYvSGXm02lol"
      },
      "outputs": [],
      "source": [
        "x_left = 147\n",
        "y_left = 499\n",
        "x_right = x_left + 1652\n",
        "y_right = y_left + 285\n",
        "\n",
        "area = (x_left, y_left, x_right, y_right)\n",
        "cropped_img = img.crop(area)\n",
        "cropped_img.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP_b9wUP3jD3"
      },
      "outputs": [],
      "source": [
        "display(cropped_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgGoL1xS39qM"
      },
      "outputs": [],
      "source": [
        "img = Image.open(\"/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_train/data/video-2FTpJaGZLDBCcJfa7-frame-001858-JqymXdTdAivdDToee.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEl1bTck4ZiZ"
      },
      "outputs": [],
      "source": [
        "display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrMYokGZ5KmJ"
      },
      "outputs": [],
      "source": [
        "x_left = 1452\n",
        "y_left = 617\n",
        "x_right = x_left + 41\n",
        "y_right = y_left + 48\n",
        "\n",
        "area = (x_left, y_left, x_right, y_right)\n",
        "cropped_img = img.crop(area)\n",
        "cropped_img.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzxJ0nJj5YxL"
      },
      "outputs": [],
      "source": [
        "display(cropped_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NqqWbyT6eyz"
      },
      "outputs": [],
      "source": [
        "shift = 128\n",
        "a = shift - 41\n",
        "b = shift - 48\n",
        "new_x_left = 1452 - a\n",
        "new_y_left = 617 - b\n",
        "new_x_right = x_left + 41 + a\n",
        "new_y_right = y_left + 48 + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC9pOYpm63iM"
      },
      "outputs": [],
      "source": [
        "area = (new_x_left, new_y_left, new_x_right, new_y_right)\n",
        "cropped_img = img.crop(area)\n",
        "cropped_img.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA5MZwZU7Bja"
      },
      "outputs": [],
      "source": [
        "display(cropped_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ1OYXxL_q02"
      },
      "outputs": [],
      "source": [
        "cropped_img.save('/content/drive/MyDrive/new.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_enlEU6QtLh"
      },
      "outputs": [],
      "source": [
        "!pip install constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBFZzArq34jt"
      },
      "outputs": [],
      "source": [
        "import constant\n",
        "constant.x = 1652\n",
        "constant.y = 1478"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWH_ZLD9RVj0"
      },
      "outputs": [],
      "source": [
        "x_left = 84\n",
        "y_left = 751\n",
        "size_x = 78\n",
        "size_y = 59"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Found max example"
      ],
      "metadata": {
        "id": "i3sjpkPotiiz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrfs9fGdkLA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b738616-a55c-4dce-99b6-ea0d83f6477f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(x_max: 256 \ty_max: 256 )\n",
            "count_256:  158550 \tcount 169174\n",
            "-----------------------------\n",
            "(x_max: 256 \ty_max: 256 )\n",
            "count_256:  15974 \tcount 16909\n",
            "-----------------------------\n",
            "(x_max: 256 \ty_max: 256 )\n",
            "count_256:  174331 \tcount 175040\n",
            "-----------------------------\n",
            "(x_max: 255 \ty_max: 256 )\n",
            "count_256:  16635 \tcount 16696\n",
            "-----------------------------\n",
            "(x_max: 256 \ty_max: 256 )\n",
            "count_256:  83661 \tcount 84786\n",
            "-----------------------------\n",
            "(x_max: 189 \ty_max: 253 )\n",
            "count_256:  62289 \tcount 62317\n",
            "-----------------------------\n"
          ]
        }
      ],
      "source": [
        "f1 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_train/coco.json')\n",
        "f2 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_val/coco.json')\n",
        "f3 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_thermal_train/coco.json')\n",
        "f4 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_thermal_val/coco.json')\n",
        "f5 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/video_rgb_test/coco.json')\n",
        "f6 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/video_thermal_test/coco.json')\n",
        "data1 = json.load(f1)\n",
        "data2 = json.load(f2)\n",
        "data3 = json.load(f3)\n",
        "data4 = json.load(f4)\n",
        "data5 = json.load(f5)\n",
        "data6 = json.load(f6)\n",
        "\n",
        "def max_coord(file_name):\n",
        "    x_max = 0\n",
        "    y_max = 0\n",
        "    count_1024 = 0\n",
        "    count = 0\n",
        "    for i in file_name[\"annotations\"]:\n",
        "        count += 1\n",
        "        if i['bbox'][2] > x_max and i['bbox'][2] <= 256:\n",
        "            x_max = i['bbox'][2]\n",
        "        if i['bbox'][2] <=256 and i['bbox'][3] <= 256:\n",
        "            count_1024 += 1\n",
        "        if i['bbox'][3] > y_max and i['bbox'][3] <= 256:\n",
        "            y_max = i['bbox'][3]\n",
        "    print('(x_max:', x_max, '\\ty_max:', y_max, ')')\n",
        "    print('count_256: ', count_1024, '\\tcount', count)\n",
        "    print('-----------------------------')\n",
        "\n",
        "max_coord(data1)\n",
        "max_coord(data2)\n",
        "max_coord(data3)\n",
        "max_coord(data4)\n",
        "max_coord(data5)\n",
        "max_coord(data6)\n",
        "\n",
        "\n",
        "f1.close()\n",
        "f2.close()\n",
        "f3.close()\n",
        "f4.close()\n",
        "f5.close()\n",
        "f6.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OVERRIDE PYCOCOTOOLS"
      ],
      "metadata": {
        "id": "fk1qf1_j5Kf1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDjvq84iBgu4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.collections import PatchCollection\n",
        "from matplotlib.patches import Polygon\n",
        "import numpy as np\n",
        "import copy\n",
        "import itertools\n",
        "import pycocotools._mask as maskUtils\n",
        "# import _mask as maskUtils\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import sys\n",
        "PYTHON_VERSION = sys.version_info[0]\n",
        "if PYTHON_VERSION == 2:\n",
        "    from urllib import urlretrieve\n",
        "elif PYTHON_VERSION == 3:\n",
        "    from urllib.request import urlretrieve\n",
        "\n",
        "\n",
        "def _isArrayLike(obj):\n",
        "    return hasattr(obj, '__iter__') and hasattr(obj, '__len__')\n",
        "\n",
        "\n",
        "class my_COCO:\n",
        "    def __init__(self, annotation_file=None):\n",
        "        \"\"\"\n",
        "        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n",
        "        :param annotation_file (str): location of annotation file\n",
        "        :param image_folder (str): location to the folder that hosts images.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # load dataset\n",
        "        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\n",
        "        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\n",
        "        if not annotation_file == None:\n",
        "            print('loading annotations into memory...')\n",
        "            tic = time.time()\n",
        "            dataset = json.load(open(annotation_file, 'r'))\n",
        "            assert type(dataset)==dict, 'annotation file format {} not supported'.format(type(dataset))\n",
        "            print('Done (t={:0.2f}s)'.format(time.time()- tic))\n",
        "            self.dataset = dataset\n",
        "            self.createIndex()\n",
        "\n",
        "    def createIndex(self):\n",
        "        # create index\n",
        "        print('creating index...')\n",
        "        anns, cats, imgs = {}, {}, {}\n",
        "        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n",
        "        if 'annotations' in self.dataset:\n",
        "            for ann in self.dataset['annotations']:\n",
        "                imgToAnns[ann['id']].append(ann)\n",
        "                anns[ann['id']] = ann\n",
        "                # ???\n",
        "                imgs[ann['id']] = ann\n",
        "\n",
        "        # if 'images' in self.dataset:\n",
        "        #     for img in self.dataset['images']:\n",
        "        #         imgs[img['id']] = img\n",
        "\n",
        "\n",
        "        if 'categories' in self.dataset:\n",
        "            for cat in self.dataset['categories']:\n",
        "                cats[cat['id']] = cat\n",
        "\n",
        "        if 'annotations' in self.dataset and 'categories' in self.dataset:\n",
        "            for ann in self.dataset['annotations']:\n",
        "                catToImgs[ann['category_id']].append(ann['id'])\n",
        "\n",
        "        print('index created!')\n",
        "\n",
        "        # create class members\n",
        "        self.anns = anns\n",
        "        self.imgToAnns = imgToAnns\n",
        "        self.catToImgs = catToImgs\n",
        "        self.imgs = imgs\n",
        "        self.cats = cats\n",
        "\n",
        "    # def info(self):\n",
        "    #     \"\"\"\n",
        "    #     Print information about the annotation file.\n",
        "    #     :return:\n",
        "    #     \"\"\"\n",
        "    #     for key, value in self.dataset['info'].items():\n",
        "    #         print('{}: {}'.format(key, value))\n",
        "\n",
        "\n",
        "# have a question abot imgs(imgIds)\n",
        "    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n",
        "        \"\"\"\n",
        "        Get ann ids that satisfy given filter conditions. default skips that filter\n",
        "        :param imgIds  (int array)     : get anns for given imgs\n",
        "               catIds  (int array)     : get anns for given cats\n",
        "               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n",
        "               iscrowd (boolean)       : get anns for given crowd label (False or True)\n",
        "        :return: ids (int array)       : integer array of ann ids\n",
        "        \"\"\"\n",
        "        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n",
        "        catIds = catIds if _isArrayLike(catIds) else [catIds]\n",
        "\n",
        "        if len(imgIds) == len(catIds) == 0:\n",
        "            anns = self.dataset['annotations']\n",
        "        else:\n",
        "            if not len(imgIds) == 0:\n",
        "                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n",
        "                anns = list(itertools.chain.from_iterable(lists))\n",
        "            else:\n",
        "                anns = self.dataset['annotations']\n",
        "            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann['category_id'] in catIds]\n",
        "            # anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann['area'] > areaRng[0] and ann['area'] < areaRng[1]]\n",
        "        if not iscrowd == None:\n",
        "            ids = [ann['id'] for ann in anns if ann['iscrowd'] == iscrowd]\n",
        "        else:\n",
        "            ids = [ann['id'] for ann in anns]\n",
        "        return ids\n",
        "\n",
        "    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n",
        "        \"\"\"\n",
        "        filtering parameters. default skips that filter.\n",
        "        :param catNms (str array)  : get cats for given cat names\n",
        "        :param supNms (str array)  : get cats for given supercategory names\n",
        "        :param catIds (int array)  : get cats for given cat ids\n",
        "        :return: ids (int array)   : integer array of cat ids\n",
        "        \"\"\"\n",
        "        catNms = catNms if _isArrayLike(catNms) else [catNms]\n",
        "        # supNms = supNms if _isArrayLike(supNms) else [supNms]\n",
        "        catIds = catIds if _isArrayLike(catIds) else [catIds]\n",
        "\n",
        "        if len(catNms) == len(catIds) == 0:\n",
        "            cats = self.dataset['categories']\n",
        "        else:\n",
        "            cats = self.dataset['categories']\n",
        "            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat['name']          in catNms]\n",
        "            # cats = cats if len(supNms) == 0 else [cat for cat in cats if cat['supercategory'] in supNms]\n",
        "            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat['id']            in catIds]\n",
        "        ids = [cat['id'] for cat in cats]\n",
        "        return ids\n",
        "\n",
        "    def getImgIds(self, imgIds=[], catIds=[]):\n",
        "        '''\n",
        "        Get img ids that satisfy given filter conditions.\n",
        "        :param imgIds (int array) : get imgs for given ids\n",
        "        :param catIds (int array) : get imgs with all given cats\n",
        "        :return: ids (int array)  : integer array of img ids\n",
        "        '''\n",
        "        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n",
        "        catIds = catIds if _isArrayLike(catIds) else [catIds]\n",
        "\n",
        "        if len(imgIds) == len(catIds) == 0:\n",
        "            ids = self.imgs.keys()\n",
        "        else:\n",
        "            ids = set(imgIds)\n",
        "            for i, catId in enumerate(catIds):\n",
        "                if i == 0 and len(ids) == 0:\n",
        "                    ids = set(self.catToImgs[catId])\n",
        "                else:\n",
        "                    ids &= set(self.catToImgs[catId])\n",
        "        return list(ids)\n",
        "\n",
        "    def cls(self, ids=[]):\n",
        "        target = [0] * 80\n",
        "        a = ([self.anns[id]['category_id'] for id in ids])\n",
        "        target[a[0] - 1] = 1\n",
        "        return target\n",
        "    \n",
        "    def loadAnns(self, ids=[]):\n",
        "        \"\"\"\n",
        "        Load anns with the specified ids.\n",
        "        :param ids (int array)       : integer ids specifying anns\n",
        "        :return: anns (object array) : loaded ann objects\n",
        "        \"\"\"\n",
        "        if _isArrayLike(ids):\n",
        "            # list_of_targets = [self.cls(ids)]\n",
        "            # np_of_targets = np.asarray(list_of_targets)\n",
        "            # return np_of_targets\n",
        "            # return [self.cls(ids)]\n",
        "            return self.cls(ids)\n",
        "            # return print(type([self.anns[id] for id in ids]))\n",
        "        elif type(ids) == int:\n",
        "            # print('int')\n",
        "            return [self.anns[ids]]\n",
        "\n",
        "    def loadCats(self, ids=[]):\n",
        "        \"\"\"\n",
        "        Load cats with the specified ids.\n",
        "        :param ids (int array)       : integer ids specifying cats\n",
        "        :return: cats (object array) : loaded cat objects\n",
        "        \"\"\"\n",
        "        if _isArrayLike(ids):\n",
        "            return [self.cats[id] for id in ids]\n",
        "        elif type(ids) == int:\n",
        "            return [self.cats[ids]]\n",
        "\n",
        "    def loadImgs(self, ids=[]):\n",
        "        \"\"\"\n",
        "        Load anns with the specified ids.\n",
        "        :param ids (int array)       : integer ids specifying img\n",
        "        :return: imgs (object array) : loaded img objects\n",
        "        \"\"\"\n",
        "        if _isArrayLike(ids):\n",
        "            return [self.imgs[id] for id in ids]\n",
        "        elif type(ids) == int:\n",
        "            return [self.imgs[ids]]\n",
        "\n",
        "    def download(self, tarDir = None, imgIds = [] ):\n",
        "        '''\n",
        "        Download COCO images from mscoco.org server.\n",
        "        :param tarDir (str): COCO results directory name\n",
        "               imgIds (list): images to be downloaded\n",
        "        :return:\n",
        "        '''\n",
        "        if tarDir is None:\n",
        "            print('Please specify target directory')\n",
        "            return -1\n",
        "        if len(imgIds) == 0:\n",
        "            imgs = self.imgs.values()\n",
        "        else:\n",
        "            imgs = self.loadImgs(imgIds)\n",
        "        N = len(imgs)\n",
        "        if not os.path.exists(tarDir):\n",
        "            os.makedirs(tarDir)\n",
        "        for i, img in enumerate(imgs):\n",
        "            tic = time.time()\n",
        "            fname = os.path.join(tarDir, img['file_name'])\n",
        "            if not os.path.exists(fname):\n",
        "                urlretrieve(img['coco_url'], fname)\n",
        "            print('downloaded {}/{} images (t={:0.1f}s)'.format(i, N, time.time()- tic))\n",
        "\n",
        "    def loadNumpyAnnotations(self, data):\n",
        "        \"\"\"\n",
        "        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n",
        "        :param  data (numpy.ndarray)\n",
        "        :return: annotations (python nested list)\n",
        "        \"\"\"\n",
        "        print('Converting ndarray to lists...')\n",
        "        assert(type(data) == np.ndarray)\n",
        "        print(data.shape)\n",
        "        assert(data.shape[1] == 7)\n",
        "        N = data.shape[0]\n",
        "        ann = []\n",
        "        for i in range(N):\n",
        "            if i % 1000000 == 0:\n",
        "                print('{}/{}'.format(i,N))\n",
        "            ann += [{\n",
        "                'image_id'  : int(data[i, 0]),\n",
        "                'bbox'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n",
        "                'score' : data[i, 5],\n",
        "                'category_id': int(data[i, 6]),\n",
        "                }]\n",
        "        return ann\n",
        "\n",
        "    def annToRLE(self, ann):\n",
        "        \"\"\"\n",
        "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
        "        :return: binary mask (numpy 2D array)\n",
        "        \"\"\"\n",
        "        t = self.imgs[ann['image_id']]\n",
        "        h, w = t['height'], t['width']\n",
        "        segm = ann['segmentation']\n",
        "        if type(segm) == list:\n",
        "            # polygon -- a single object might consist of multiple parts\n",
        "            # we merge all parts into one mask rle code\n",
        "            rles = maskUtils.frPyObjects(segm, h, w)\n",
        "            rle = maskUtils.merge(rles)\n",
        "        elif type(segm['counts']) == list:\n",
        "            # uncompressed RLE\n",
        "            rle = maskUtils.frPyObjects(segm, h, w)\n",
        "        else:\n",
        "            # rle\n",
        "            rle = ann['segmentation']\n",
        "        return rle\n",
        "\n",
        "    def annToMask(self, ann):\n",
        "        \"\"\"\n",
        "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
        "        :return: binary mask (numpy 2D array)\n",
        "        \"\"\"\n",
        "        rle = self.annToRLE(ann)\n",
        "        m = maskUtils.decode(rle)\n",
        "        return m\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class my_CocoDetection(VisionDataset):\n",
        "    \"\"\"`MS Coco Detection <https://cocodataset.org/#detection-2016>`_ Dataset.\n",
        "\n",
        "    It requires the `COCO API to be installed <https://github.com/pdollar/coco/tree/master/PythonAPI>`_.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory where images are downloaded to.\n",
        "        annFile (string): Path to json annotation file.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
        "            and returns a transformed version.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        annFile: str,\n",
        "        transform: Optional[Callable] = None,\n",
        "        target_transform: Optional[Callable] = None,\n",
        "        transforms: Optional[Callable] = None,\n",
        "    ) -> None:\n",
        "        super().__init__(root, transforms, transform, target_transform)\n",
        "        from pycocotools.coco import COCO\n",
        "\n",
        "        self.coco = my_COCO(annFile)\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "\n",
        "    def _load_image(self, id: int) -> Image.Image:\n",
        "        path = self.coco.loadImgs(id)[0][\"file_name\"]\n",
        "        # im = Image.open(os.path.join(self.root, os.path.basename(path))).convert(\"RGB\")\n",
        "        # display(im)\n",
        "        return Image.open(os.path.join(self.root, os.path.basename(path)))\n",
        "\n",
        "    def _load_target(self, id: int):\n",
        "        # target = self.coco.loadAnns(self.coco.getAnnIds(id))\n",
        "        # if self.target_transform is not None:\n",
        "        #   target = self.target_transform(target)\n",
        "        # return target\n",
        "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "        id = self.ids[index]\n",
        "        image = self._load_image(id)\n",
        "        # target = self._load_target(id)\n",
        "        target = torch.as_tensor(self._load_target(id), dtype = torch.float32)\n",
        "        # print(type(image))\n",
        "        if self.transforms is not None:\n",
        "            image, target = self.transforms(image, target)\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.ids)\n",
        "    \n",
        "    def printf(self):\n",
        "      print(self.root)\n"
      ],
      "metadata": {
        "id": "-8hzSbkV2-2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = Compose([PILToTensor(), ConvertImageDtype(torch.float)])"
      ],
      "metadata": {
        "id": "gmo4Y7ulsru7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path2data=\"/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_val/new_data\"\n",
        "path2json=\"/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_val/ann.json\""
      ],
      "metadata": {
        "id": "Vbvi8VRPQJzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coco = my_CocoDetection(path2data, path2json, transform = PILToTensor())\n",
        "coco = my_CocoDetection(path2data, path2json, transform = transform)\n",
        "# coco = my_CocoDetection(path2data, path2json, transforms=ToTensor())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkVl4Br8QhvX",
        "outputId": "45440a06-6bc9-4cec-e697-832b86a886b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.04s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coco"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNFIVOO02MOS",
        "outputId": "284504ed-5adb-42b0-f109-6400aa98aa78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset my_CocoDetection\n",
              "    Number of datapoints: 15974\n",
              "    Root location: /content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_val/new_data\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               PILToTensor()\n",
              "               ConvertImageDtype()\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coco.__getitem__(58)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a85CLFvrdjV",
        "outputId": "c58d7889-9ff6-48bf-8682-84404a3b7e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.5216, 0.5137, 0.5020,  ..., 0.5529, 0.5490, 0.5490],\n",
              "          [0.5020, 0.4980, 0.4863,  ..., 0.5333, 0.5373, 0.5412],\n",
              "          [0.4941, 0.4902, 0.4824,  ..., 0.5137, 0.5255, 0.5294],\n",
              "          ...,\n",
              "          [0.8039, 0.8039, 0.8039,  ..., 0.7843, 0.7843, 0.7843],\n",
              "          [0.7922, 0.7922, 0.7922,  ..., 0.7843, 0.7843, 0.7843],\n",
              "          [0.7882, 0.7882, 0.7882,  ..., 0.7882, 0.7843, 0.7843]],\n",
              " \n",
              "         [[0.5333, 0.5216, 0.5098,  ..., 0.5020, 0.4980, 0.4980],\n",
              "          [0.5098, 0.5059, 0.4941,  ..., 0.4941, 0.4980, 0.5020],\n",
              "          [0.5020, 0.4980, 0.4902,  ..., 0.4824, 0.4941, 0.4980],\n",
              "          ...,\n",
              "          [0.8118, 0.8118, 0.8118,  ..., 0.7843, 0.7843, 0.7843],\n",
              "          [0.8078, 0.8078, 0.8078,  ..., 0.7843, 0.7843, 0.7843],\n",
              "          [0.8039, 0.8039, 0.8039,  ..., 0.7882, 0.7843, 0.7843]],\n",
              " \n",
              "         [[0.4667, 0.4667, 0.4588,  ..., 0.4667, 0.4627, 0.4627],\n",
              "          [0.4549, 0.4510, 0.4431,  ..., 0.4549, 0.4588, 0.4627],\n",
              "          [0.4471, 0.4431, 0.4392,  ..., 0.4392, 0.4510, 0.4549],\n",
              "          ...,\n",
              "          [0.8078, 0.8078, 0.8078,  ..., 0.7765, 0.7765, 0.7765],\n",
              "          [0.8039, 0.8039, 0.8039,  ..., 0.7765, 0.7765, 0.7765],\n",
              "          [0.8000, 0.8000, 0.8000,  ..., 0.7804, 0.7765, 0.7765]]]),\n",
              " tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0.]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coco._load_target(12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9moHjDOOwFB",
        "outputId": "1feb4b83-e870-488c-f803-5b1e33448260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 3"
      ],
      "metadata": {
        "id": "fodFyDoUvMib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco_loader = DataLoader(coco, batch_size, shuffle=True, num_workers = 0)"
      ],
      "metadata": {
        "id": "NgOlWxuUvI3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "metadata": {
        "id": "Sl9rFccegmBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as dset"
      ],
      "metadata": {
        "id": "3iyiIZQHg7bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_default_device()\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro902skDh3Ru",
        "outputId": "2f4538ce-7abc-4f65-cc0c-39e35bf53081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    # rewrite\n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        labels1 = labels.softmax(dim=1)\n",
        "        # print(labels, labels.size())\n",
        "        print(labels)\n",
        "        print(out)\n",
        "        # print (out, out.size())\n",
        "        loss = F.cross_entropy(out, labels1)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "      \n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
      ],
      "metadata": {
        "id": "2OJmfBUlh5ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet34 = models.resnet34()\n",
        "resnet34"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvcheV5yh6tu",
        "outputId": "ea3151e4-5997-41c4-f92e-64ec71cf978b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FLIR80Resnet(ImageClassificationBase):\n",
        "        def __init__(self):\n",
        "                super().__init__()\n",
        "                self.network = torchvision.models.resnet34()\n",
        "                num_ftrs = self.network.fc.in_features\n",
        "                self.network.fc = nn.Linear(num_ftrs, 80)\n",
        "        \n",
        "        def forward(self, xb):\n",
        "                return self.network(xb)\n",
        "            \n",
        "model = FLIR80Resnet()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o8Pcu1fh9tS",
        "outputId": "67ffd05a-dad9-4ac7-d72f-a39dad5bf583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FLIR80Resnet(\n",
              "  (network): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (5): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(in_features=512, out_features=80, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = to_device(FLIR80Resnet(), device)"
      ],
      "metadata": {
        "id": "bHXW8F4DiB-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, \n",
        "        weight_decay=0, opt_func=torch.optim.SGD):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr, weight_decay=weight_decay)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ],
      "metadata": {
        "id": "MXlr4vGAiGSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = [evaluate(model, coco_loader)]\n",
        "history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yuQp9Ht2iI4W",
        "outputId": "38e1160b-d25b-4a05-fa47-636b005b5865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[-28.6605, -23.5321, -27.5766, -31.6898,  -5.8626,  26.5057, -13.1705,\n",
            "         -12.3365,  15.8269,   9.1745, -13.5935, -22.1579,   1.5741,  33.8009,\n",
            "          26.4971,  13.8649,  19.1690,  -7.3294,   6.3370,  20.4607,  -0.8527,\n",
            "          -9.2501,  -2.4055,   3.8635,  31.2777,  12.9858,  16.2673,   0.2877,\n",
            "         -17.5756,  -9.5369, -16.6597,  -0.1314,  17.1969,  10.5726,   4.8003,\n",
            "           5.6214,   4.7214,  -5.0233,   2.3219,  19.1363,  23.6661,   1.9188,\n",
            "          16.8010,   9.6572, -22.1001,  24.9362,   1.2269,  22.6096,  -7.0476,\n",
            "           1.4277,  24.7951,   4.0582,   3.8546, -19.7898,   1.9556, -24.0996,\n",
            "           9.5793,  10.2152, -13.5691,   1.3912,  28.8416, -18.7087, -36.7362,\n",
            "          -6.8695,   7.6969,   2.4617,  -1.1510,  -3.4584,  25.4822,  -8.1430,\n",
            "          -4.2906,  11.2371, -11.2434,  -5.8081,  30.7875,   3.1390,  10.3111,\n",
            "          40.0544, -11.8759,  38.0311],\n",
            "        [-26.7337, -22.7776, -25.9887, -29.5996,  -5.4232,  24.9896, -12.5203,\n",
            "         -11.5021,  14.8036,   9.0768, -13.4076, -20.5717,   0.9357,  32.3026,\n",
            "          25.2658,  13.6365,  18.3660,  -6.7299,   6.0242,  19.9652,  -0.6359,\n",
            "          -8.7987,  -1.3304,   3.6181,  30.0454,  11.8401,  15.2335,   0.6456,\n",
            "         -16.5177,  -8.4040, -16.2376,   0.1241,  16.3145,   9.7451,   4.5536,\n",
            "           5.3164,   4.8521,  -4.7907,   2.7800,  17.7188,  22.7214,   1.5024,\n",
            "          15.9868,   9.4854, -20.6186,  22.8332,   1.2807,  22.4702,  -7.0342,\n",
            "           0.6620,  23.7090,   4.4805,   3.6020, -19.2285,   1.8712, -22.2875,\n",
            "           9.4326,   9.8570, -12.9348,   0.6742,  27.2871, -17.1267, -34.4788,\n",
            "          -6.8910,   7.2648,   1.7210,  -1.0458,  -3.3328,  23.8280,  -8.4452,\n",
            "          -3.7716,  10.2696, -10.9501,  -5.5665,  28.7967,   2.1412,  10.1793,\n",
            "          37.9920, -11.4015,  35.7926],\n",
            "        [-28.8892, -23.6482, -27.5935, -31.5937,  -5.8339,  26.0297, -13.1534,\n",
            "         -11.8197,  15.6435,   9.1969, -13.6457, -22.2068,   1.4581,  33.5259,\n",
            "          26.6445,  14.2456,  19.7323,  -7.2269,   6.3220,  20.7270,  -0.0691,\n",
            "          -9.3811,  -2.2656,   3.7279,  30.6632,  12.6865,  16.2065,   0.1253,\n",
            "         -17.3555,  -9.7638, -17.2341,  -0.3264,  16.9547,  10.4239,   4.7800,\n",
            "           5.5692,   4.8703,  -5.1667,   2.1424,  19.1391,  23.7990,   1.4867,\n",
            "          16.7273,   9.8315, -21.9953,  24.6708,   1.5827,  22.5562,  -7.4764,\n",
            "           1.6702,  24.9335,   3.8211,   3.9765, -20.3880,   1.7434, -23.9042,\n",
            "           9.6863,  10.4364, -13.5027,   1.0716,  28.3744, -18.9083, -36.8328,\n",
            "          -6.6702,   8.0544,   2.4159,  -1.0194,  -3.1818,  25.2210,  -8.8446,\n",
            "          -4.0302,  11.4021, -10.7858,  -6.3507,  30.2713,   3.3959,  10.3173,\n",
            "          40.1243, -11.7561,  37.7870]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-5158a52f24e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoco_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-f433c3f1ec82>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, val_loader)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-f433c3f1ec82>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-fc2a73bf284f>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# print (out, out.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels1\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# Calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-fc2a73bf284f>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(outputs, labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mImageClassificationBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (80) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.randn(3, 5).softmax(dim=1)\n",
        "print(input)\n",
        "print(target)\n",
        "loss = F.cross_entropy(input, target)\n",
        "loss.backward()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6owSf_U2upM",
        "outputId": "40e984e8-f5a1-4b67-8048-5e8d625dc0c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.6092,  1.5785,  0.3189,  0.5635, -0.0618],\n",
            "        [-1.0746, -1.8930,  1.2565, -0.7404, -0.8455],\n",
            "        [-0.9029, -0.0176,  0.2300, -0.2433, -0.1535]], requires_grad=True)\n",
            "tensor([[0.0612, 0.2935, 0.2335, 0.0501, 0.3618],\n",
            "        [0.0497, 0.0884, 0.6734, 0.0731, 0.1155],\n",
            "        [0.4664, 0.2580, 0.0399, 0.0979, 0.1377]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path2data=\"/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_val/data\"\n",
        "path2json=\"/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_val/coco.json\""
      ],
      "metadata": {
        "id": "lBbwqetTX36N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco_train_detect = my_CocoDetection(path2data, path2json, transform=PILToTensor())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaX3LV9X3DZX",
        "outputId": "3587a044-cb5a-4e0b-e568-e10ef7a37a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=2.26s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# return all targets from image_id (arguments = 'image_id')\n",
        "coco_train_detect.__getitem__(1)"
      ],
      "metadata": {
        "id": "MadU3nN16O9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco_train_detect._load_target(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ONuRX3TAWOF",
        "outputId": "7269932c-5d96-4b7a-e801-db180a6fc1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'area': 110,\n",
              "  'bbox': [360, 217, 10, 11],\n",
              "  'category_id': 12,\n",
              "  'extra_info': {'human_annotated': 'human', 'occluded': 'no_(fully_visible)'},\n",
              "  'id': 16,\n",
              "  'image_id': 1,\n",
              "  'iscrowd': False,\n",
              "  'segmentation': [[360, 217, 370, 217, 360, 228, 370, 228]]},\n",
              " {'area': 189,\n",
              "  'bbox': [199, 223, 21, 9],\n",
              "  'category_id': 12,\n",
              "  'extra_info': {'human_annotated': 'human',\n",
              "   'occluded': '1%_-_70%_occluded_(partially_occluded)'},\n",
              "  'id': 17,\n",
              "  'image_id': 1,\n",
              "  'iscrowd': False,\n",
              "  'segmentation': [[199, 223, 220, 223, 199, 232, 220, 232]]},\n",
              " {'area': 602,\n",
              "  'bbox': [489, 222, 14, 43],\n",
              "  'category_id': 1,\n",
              "  'extra_info': {'human_annotated': 'human',\n",
              "   'occluded': 'no_(fully_visible)',\n",
              "   'specify_if_human_or_not': 'human'},\n",
              "  'id': 18,\n",
              "  'image_id': 1,\n",
              "  'iscrowd': False,\n",
              "  'segmentation': [[489, 222, 503, 222, 489, 265, 503, 265]]},\n",
              " {'area': 748,\n",
              "  'bbox': [421, 245, 34, 22],\n",
              "  'category_id': 2,\n",
              "  'extra_info': {'human_annotated': 'human',\n",
              "   'occluded': '1%_-_70%_occluded_(partially_occluded)'},\n",
              "  'id': 19,\n",
              "  'image_id': 1,\n",
              "  'iscrowd': False,\n",
              "  'segmentation': [[421, 245, 455, 245, 421, 267, 455, 267]]},\n",
              " {'area': 8449,\n",
              "  'bbox': [75, 228, 119, 71],\n",
              "  'category_id': 3,\n",
              "  'extra_info': {'human_annotated': 'human', 'occluded': 'no_(fully_visible)'},\n",
              "  'id': 20,\n",
              "  'image_id': 1,\n",
              "  'iscrowd': False,\n",
              "  'segmentation': [[75, 228, 194, 228, 75, 299, 194, 299]]},\n",
              " {'area': 2632,\n",
              "  'bbox': [271, 224, 56, 47],\n",
              "  'category_id': 3,\n",
              "  'extra_info': {'human_annotated': 'human', 'occluded': 'no_(fully_visible)'},\n",
              "  'id': 21,\n",
              "  'image_id': 1,\n",
              "  'iscrowd': False,\n",
              "  'segmentation': [[271, 224, 327, 224, 271, 271, 327, 271]]},\n",
              " {'area': 475,\n",
              "  'bbox': [235, 236, 25, 19],\n",
              "  'category_id': 3,\n",
              "  'extra_info': {'human_annotated': 'human', 'occluded': 'no_(fully_visible)'},\n",
              "  'id': 22,\n",
              "  'image_id': 1,\n",
              "  'iscrowd': False,\n",
              "  'segmentation': [[235, 236, 260, 236, 235, 255, 260, 255]]}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bl9xPD8XPKeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create new catalog with objects "
      ],
      "metadata": {
        "id": "iMhmeDP04JZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def max_coord(file_name):\n",
        "    x_max = 0\n",
        "    y_max = 0\n",
        "    count_1024 = 0\n",
        "    count = 0\n",
        "    for i in file_name[\"annotations\"]:\n",
        "        count += 1\n",
        "        if i['bbox'][2] > x_max and i['bbox'][2] <= 256:\n",
        "            x_max = i['bbox'][2]\n",
        "        if i['bbox'][2] <=256 and i['bbox'][3] <= 256:\n",
        "            count_1024 += 1\n",
        "        if i['bbox'][3] > y_max and i['bbox'][3] <= 256:\n",
        "            y_max = i['bbox'][3]\n",
        "    print('(x_max:', x_max, '\\ty_max:', y_max, ')')\n",
        "    print('count_256: ', count_1024, '\\tcount', count)\n",
        "    print('-----------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXHTi0m04HSi",
        "outputId": "b2916ce5-11b2-4cba-9b96-8af59b28cce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(x_max: 256 \ty_max: 256 )\n",
            "count_256:  158550 \tcount 169174\n",
            "-----------------------------\n",
            "(x_max: 256 \ty_max: 256 )\n",
            "count_256:  15974 \tcount 16909\n",
            "-----------------------------\n",
            "(x_max: 256 \ty_max: 256 )\n",
            "count_256:  174331 \tcount 175040\n",
            "-----------------------------\n",
            "(x_max: 255 \ty_max: 256 )\n",
            "count_256:  16635 \tcount 16696\n",
            "-----------------------------\n",
            "(x_max: 256 \ty_max: 256 )\n",
            "count_256:  83661 \tcount 84786\n",
            "-----------------------------\n",
            "(x_max: 189 \ty_max: 253 )\n",
            "count_256:  62289 \tcount 62317\n",
            "-----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Old code without overriding annotation"
      ],
      "metadata": {
        "id": "fxauclQ0gspX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_img(path, file_name):\n",
        "  shift = 256\n",
        "  x_left = 0\n",
        "  y_left = 0\n",
        "  x_right = 0\n",
        "  y_right = 0\n",
        "  for i in file_name[\"annotations\"]:\n",
        "    if (i['bbox'][2] <= shift and i['bbox'][3] <= shift):\n",
        "      if i['bbox'][2] % 2 == 1:\n",
        "        i['bbox'][2] += 1\n",
        "      if i['bbox'][3] % 2 == 1:\n",
        "        i['bbox'][3] += 1\n",
        "      a = int((shift - i['bbox'][2])/2)\n",
        "      b = int((shift - i['bbox'][3])/2)\n",
        "      x_left = i['bbox'][0] - a\n",
        "      y_left = i['bbox'][1] - b\n",
        "      x_right = i['bbox'][0] + i['bbox'][2] + a\n",
        "      y_right = i['bbox'][1] + i['bbox'][3] + b\n",
        "      for j in file_name[\"images\"]:\n",
        "        if i['image_id'] == j['id']:\n",
        "          img = Image.open(path + j['file_name'])\n",
        "          area = (x_left, y_left, x_right, y_right)\n",
        "          cropped_img = img.crop(area)\n",
        "          cropped_img.save(str(path + 'new_data/' + (os.path.splitext(os.path.basename(j['file_name'])))[0] + '_obj' + str(i['id']) + '.jpg'))\n",
        "          break"
      ],
      "metadata": {
        "id": "o-6157Nb807J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "New code with overriding annotation"
      ],
      "metadata": {
        "id": "oND9uB_bg0SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def write_json(new_data, new_data2, new_data3, new_data4, new_data5, new_data6,  filename = 'ann.json'):\n",
        "def write_json_ann(new_data, new_data2, new_data3, new_data4, new_data5, new_data6,  path):\n",
        "    filename = path + 'ann.json'\n",
        "    with open(filename, 'r+') as file:\n",
        "        # First we load existing data into a dict.\n",
        "        file_data = json.load(file)\n",
        "        # Join new_data with file_data inside annotation\n",
        "        new = {new_data: new_data2, new_data3: new_data4, new_data5: new_data6}\n",
        "        file_data[\"annotations\"].append(new)\n",
        "        # Sets file's current position at offset.\n",
        "        file.seek(0)\n",
        "        # convert back to json.\n",
        "        json.dump(file_data, file, indent = 4)"
      ],
      "metadata": {
        "id": "YEDuS-NIfFKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def write_json(new_data, new_data2, new_data3, new_data4, filename = 'ann.json'):\n",
        "def write_json_cat(new_data, new_data2, new_data3, new_data4, path):\n",
        "    filename = path + 'ann.json'\n",
        "    with open(filename, 'r+') as file:\n",
        "        file_data = json.load(file)\n",
        "        new = {new_data: new_data2, new_data3: new_data4}\n",
        "        file_data[\"categories\"].append(new)\n",
        "        file.seek(0)\n",
        "        json.dump(file_data, file, indent = 4)"
      ],
      "metadata": {
        "id": "zd-3oJOofcOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_img_with_ann(path, file_name):\n",
        "  shift = 256\n",
        "  x_left = 0\n",
        "  y_left = 0\n",
        "  x_right = 0\n",
        "  y_right = 0\n",
        "  for i in file_name[\"annotations\"]:\n",
        "    if (i['bbox'][2] <= 256 and i['bbox'][3] <= 256):\n",
        "      if i['bbox'][2] % 2 == 1:\n",
        "        i['bbox'][2] += 1\n",
        "      if i['bbox'][3] % 2 == 1:\n",
        "        i['bbox'][3] += 1\n",
        "      a = int((shift - i['bbox'][2])/2)\n",
        "      b = int((shift - i['bbox'][3])/2)\n",
        "      x_left = i['bbox'][0] - a\n",
        "      y_left = i['bbox'][1] - b\n",
        "      x_right = i['bbox'][0] + i['bbox'][2] + a\n",
        "      y_right = i['bbox'][1] + i['bbox'][3] + b\n",
        "      for j in file_name[\"images\"]:\n",
        "        if i['image_id'] == j['id']:\n",
        "          img = Image.open(path + j['file_name'])\n",
        "          area = (x_left, y_left, x_right, y_right)\n",
        "          cropped_img = img.crop(area)\n",
        "          cropped_img.save(str(path + 'new_data/' + (os.path.splitext(os.path.basename(j['file_name'])))[0] + '_obj' + str(i['id']) + '.jpg'))\n",
        "          write_json_ann(\"id\", i['id'], \"category_id\", i['category_id'], \"file_name\", str(path + 'new_data/' + (os.path.splitext(os.path.basename(j['file_name'])))[0] + '_obj' + str(i['id']) + '.jpg'), path)\n",
        "          break\n",
        "  for i in file_name[\"categories\"]:\n",
        "    write_json_cat(\"id\", i['id'], \"name\", i['name'], path)"
      ],
      "metadata": {
        "id": "IQVrbmCkg4qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_train/'\n",
        "f1 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_train/coco.json')\n",
        "f2 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_val/coco.json')\n",
        "f3 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_thermal_train/coco.json')\n",
        "f4 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_thermal_val/coco.json')\n",
        "f5 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/video_rgb_test/coco.json')\n",
        "f6 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/video_thermal_test/coco.json')\n",
        "\n",
        "data1 = json.load(f1)\n",
        "data2 = json.load(f2)\n",
        "data3 = json.load(f3)\n",
        "data4 = json.load(f4)\n",
        "data5 = json.load(f5)\n",
        "data6 = json.load(f6)\n",
        "\n",
        "max_coord(data1)\n",
        "max_coord(data2)\n",
        "max_coord(data3)\n",
        "max_coord(data4)\n",
        "max_coord(data5)\n",
        "max_coord(data6)\n",
        "\n",
        "f1.close()\n",
        "f2.close()\n",
        "f3.close()\n",
        "f4.close()\n",
        "f5.close()\n",
        "f6.close()"
      ],
      "metadata": {
        "id": "lEwsXe2S8sKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_val/'\n",
        "f1 = open(path + 'coco.json')\n",
        "data1 = json.load(f1)\n",
        "crop_img(path, data1)\n",
        "f1.close()"
      ],
      "metadata": {
        "id": "38dqJraXgZyS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "lQUBXJjztVC9",
        "i3sjpkPotiiz"
      ],
      "name": "crop_picture",
      "provenance": [],
      "mount_file_id": "11B5Ovru1G3AkRLh4A-6OOGpjgGlZ6Gi2",
      "authorship_tag": "ABX9TyNXnJaVGBp+LHE+nVbn9Iii",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}