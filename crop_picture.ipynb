{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/e-margot/FLIR80/blob/main/crop_picture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H8ZrwcxmdWZ"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "from torchvision.transforms import ToTensor, PILToTensor, ConvertImageDtype, Compose\n",
        "import os.path\n",
        "from typing import Any, Callable, Optional, Tuple, List\n",
        "from torchvision.datasets.vision import VisionDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor, PILToTensor\n",
        "import torchvision.transforms as tt\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "cBIjeobYg0Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-ya92N6u3b-",
        "outputId": "bd41cc4e-ee96-4843-bac3-31eb66b0dde0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crop picture example "
      ],
      "metadata": {
        "id": "lQUBXJjztVC9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LKP_ZeDu1rV"
      },
      "outputs": [],
      "source": [
        "img = Image.open(\"/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_train/data/video-4WXE26tzNxT688hT3-frame-002055-7Ghf3TZLA3GX5fWeT.jpg\")\n",
        "width, height = img.size\n",
        "print(width, height)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oArgFt_WwQLm"
      },
      "outputs": [],
      "source": [
        "display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shGfGccBvjzE"
      },
      "outputs": [],
      "source": [
        "x_left = 147\n",
        "y_left = 499\n",
        "x_right = x_left + 1652\n",
        "y_right = y_left + 285\n",
        "\n",
        "area = (x_left, y_left, x_right, y_right)\n",
        "cropped_img = img.crop(area)\n",
        "cropped_img.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73BDubW13Zkf"
      },
      "outputs": [],
      "source": [
        "display(cropped_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYvSGXm02lol"
      },
      "outputs": [],
      "source": [
        "x_left = 147\n",
        "y_left = 499\n",
        "x_right = x_left + 1652\n",
        "y_right = y_left + 285\n",
        "\n",
        "area = (x_left, y_left, x_right, y_right)\n",
        "cropped_img = img.crop(area)\n",
        "cropped_img.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP_b9wUP3jD3"
      },
      "outputs": [],
      "source": [
        "display(cropped_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgGoL1xS39qM"
      },
      "outputs": [],
      "source": [
        "img = Image.open(\"/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_train/data/video-2FTpJaGZLDBCcJfa7-frame-001858-JqymXdTdAivdDToee.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEl1bTck4ZiZ"
      },
      "outputs": [],
      "source": [
        "display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrMYokGZ5KmJ"
      },
      "outputs": [],
      "source": [
        "x_left = 1452\n",
        "y_left = 617\n",
        "x_right = x_left + 41\n",
        "y_right = y_left + 48\n",
        "\n",
        "area = (x_left, y_left, x_right, y_right)\n",
        "cropped_img = img.crop(area)\n",
        "cropped_img.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzxJ0nJj5YxL"
      },
      "outputs": [],
      "source": [
        "display(cropped_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NqqWbyT6eyz"
      },
      "outputs": [],
      "source": [
        "shift = 128\n",
        "a = shift - 41\n",
        "b = shift - 48\n",
        "new_x_left = 1452 - a\n",
        "new_y_left = 617 - b\n",
        "new_x_right = x_left + 41 + a\n",
        "new_y_right = y_left + 48 + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC9pOYpm63iM"
      },
      "outputs": [],
      "source": [
        "area = (new_x_left, new_y_left, new_x_right, new_y_right)\n",
        "cropped_img = img.crop(area)\n",
        "cropped_img.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA5MZwZU7Bja"
      },
      "outputs": [],
      "source": [
        "display(cropped_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ1OYXxL_q02"
      },
      "outputs": [],
      "source": [
        "cropped_img.save('/content/drive/MyDrive/new.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_enlEU6QtLh"
      },
      "outputs": [],
      "source": [
        "!pip install constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBFZzArq34jt"
      },
      "outputs": [],
      "source": [
        "import constant\n",
        "constant.x = 1652\n",
        "constant.y = 1478"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWH_ZLD9RVj0"
      },
      "outputs": [],
      "source": [
        "x_left = 84\n",
        "y_left = 751\n",
        "size_x = 78\n",
        "size_y = 59"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Found max example"
      ],
      "metadata": {
        "id": "i3sjpkPotiiz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrfs9fGdkLA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b738616-a55c-4dce-99b6-ea0d83f6477f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(x_max: 256 \ty_max: 256 )\n",
            "count_256:  158550 \tcount 169174\n",
            "-----------------------------\n",
            "(x_max: 256 \ty_max: 256 )\n",
            "count_256:  15974 \tcount 16909\n",
            "-----------------------------\n",
            "(x_max: 256 \ty_max: 256 )\n",
            "count_256:  174331 \tcount 175040\n",
            "-----------------------------\n",
            "(x_max: 255 \ty_max: 256 )\n",
            "count_256:  16635 \tcount 16696\n",
            "-----------------------------\n",
            "(x_max: 256 \ty_max: 256 )\n",
            "count_256:  83661 \tcount 84786\n",
            "-----------------------------\n",
            "(x_max: 189 \ty_max: 253 )\n",
            "count_256:  62289 \tcount 62317\n",
            "-----------------------------\n"
          ]
        }
      ],
      "source": [
        "f1 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_train/coco.json')\n",
        "f2 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_val/coco.json')\n",
        "f3 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_thermal_train/coco.json')\n",
        "f4 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_thermal_val/coco.json')\n",
        "f5 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/video_rgb_test/coco.json')\n",
        "f6 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/video_thermal_test/coco.json')\n",
        "data1 = json.load(f1)\n",
        "data2 = json.load(f2)\n",
        "data3 = json.load(f3)\n",
        "data4 = json.load(f4)\n",
        "data5 = json.load(f5)\n",
        "data6 = json.load(f6)\n",
        "\n",
        "def max_coord(file_name):\n",
        "    x_max = 0\n",
        "    y_max = 0\n",
        "    count_1024 = 0\n",
        "    count = 0\n",
        "    for i in file_name[\"annotations\"]:\n",
        "        count += 1\n",
        "        if i['bbox'][2] > x_max and i['bbox'][2] <= 256:\n",
        "            x_max = i['bbox'][2]\n",
        "        if i['bbox'][2] <=256 and i['bbox'][3] <= 256:\n",
        "            count_1024 += 1\n",
        "        if i['bbox'][3] > y_max and i['bbox'][3] <= 256:\n",
        "            y_max = i['bbox'][3]\n",
        "    print('(x_max:', x_max, '\\ty_max:', y_max, ')')\n",
        "    print('count_256: ', count_1024, '\\tcount', count)\n",
        "    print('-----------------------------')\n",
        "\n",
        "max_coord(data1)\n",
        "max_coord(data2)\n",
        "max_coord(data3)\n",
        "max_coord(data4)\n",
        "max_coord(data5)\n",
        "max_coord(data6)\n",
        "\n",
        "\n",
        "f1.close()\n",
        "f2.close()\n",
        "f3.close()\n",
        "f4.close()\n",
        "f5.close()\n",
        "f6.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OVERRIDE PYCOCOTOOLS"
      ],
      "metadata": {
        "id": "fk1qf1_j5Kf1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDjvq84iBgu4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.collections import PatchCollection\n",
        "from matplotlib.patches import Polygon\n",
        "import numpy as np\n",
        "import copy\n",
        "import itertools\n",
        "import pycocotools._mask as maskUtils\n",
        "# import _mask as maskUtils\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import sys\n",
        "PYTHON_VERSION = sys.version_info[0]\n",
        "if PYTHON_VERSION == 2:\n",
        "    from urllib import urlretrieve\n",
        "elif PYTHON_VERSION == 3:\n",
        "    from urllib.request import urlretrieve\n",
        "\n",
        "\n",
        "def _isArrayLike(obj):\n",
        "    return hasattr(obj, '__iter__') and hasattr(obj, '__len__')\n",
        "\n",
        "\n",
        "class my_COCO:\n",
        "    def __init__(self, annotation_file=None):\n",
        "        \"\"\"\n",
        "        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n",
        "        :param annotation_file (str): location of annotation file\n",
        "        :param image_folder (str): location to the folder that hosts images.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # load dataset\n",
        "        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\n",
        "        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\n",
        "        if not annotation_file == None:\n",
        "            print('loading annotations into memory...')\n",
        "            tic = time.time()\n",
        "            dataset = json.load(open(annotation_file, 'r'))\n",
        "            assert type(dataset)==dict, 'annotation file format {} not supported'.format(type(dataset))\n",
        "            print('Done (t={:0.2f}s)'.format(time.time()- tic))\n",
        "            self.dataset = dataset\n",
        "            self.createIndex()\n",
        "\n",
        "    def createIndex(self):\n",
        "        # create index\n",
        "        print('creating index...')\n",
        "        anns, cats, imgs = {}, {}, {}\n",
        "        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n",
        "        if 'annotations' in self.dataset:\n",
        "            for ann in self.dataset['annotations']:\n",
        "                imgToAnns[ann['id']].append(ann)\n",
        "                anns[ann['id']] = ann\n",
        "                # ???\n",
        "                imgs[ann['id']] = ann\n",
        "\n",
        "        # if 'images' in self.dataset:\n",
        "        #     for img in self.dataset['images']:\n",
        "        #         imgs[img['id']] = img\n",
        "\n",
        "\n",
        "        if 'categories' in self.dataset:\n",
        "            for cat in self.dataset['categories']:\n",
        "                cats[cat['id']] = cat\n",
        "\n",
        "        if 'annotations' in self.dataset and 'categories' in self.dataset:\n",
        "            for ann in self.dataset['annotations']:\n",
        "                catToImgs[ann['category_id']].append(ann['id'])\n",
        "\n",
        "        print('index created!')\n",
        "\n",
        "        # create class members\n",
        "        self.anns = anns\n",
        "        self.imgToAnns = imgToAnns\n",
        "        self.catToImgs = catToImgs\n",
        "        self.imgs = imgs\n",
        "        self.cats = cats\n",
        "\n",
        "    # def info(self):\n",
        "    #     \"\"\"\n",
        "    #     Print information about the annotation file.\n",
        "    #     :return:\n",
        "    #     \"\"\"\n",
        "    #     for key, value in self.dataset['info'].items():\n",
        "    #         print('{}: {}'.format(key, value))\n",
        "\n",
        "\n",
        "# have a question abot imgs(imgIds)\n",
        "    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n",
        "        \"\"\"\n",
        "        Get ann ids that satisfy given filter conditions. default skips that filter\n",
        "        :param imgIds  (int array)     : get anns for given imgs\n",
        "               catIds  (int array)     : get anns for given cats\n",
        "               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n",
        "               iscrowd (boolean)       : get anns for given crowd label (False or True)\n",
        "        :return: ids (int array)       : integer array of ann ids\n",
        "        \"\"\"\n",
        "        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n",
        "        catIds = catIds if _isArrayLike(catIds) else [catIds]\n",
        "\n",
        "        if len(imgIds) == len(catIds) == 0:\n",
        "            anns = self.dataset['annotations']\n",
        "        else:\n",
        "            if not len(imgIds) == 0:\n",
        "                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n",
        "                anns = list(itertools.chain.from_iterable(lists))\n",
        "            else:\n",
        "                anns = self.dataset['annotations']\n",
        "            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann['category_id'] in catIds]\n",
        "            # anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann['area'] > areaRng[0] and ann['area'] < areaRng[1]]\n",
        "        if not iscrowd == None:\n",
        "            ids = [ann['id'] for ann in anns if ann['iscrowd'] == iscrowd]\n",
        "        else:\n",
        "            ids = [ann['id'] for ann in anns]\n",
        "        return ids\n",
        "\n",
        "    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n",
        "        \"\"\"\n",
        "        filtering parameters. default skips that filter.\n",
        "        :param catNms (str array)  : get cats for given cat names\n",
        "        :param supNms (str array)  : get cats for given supercategory names\n",
        "        :param catIds (int array)  : get cats for given cat ids\n",
        "        :return: ids (int array)   : integer array of cat ids\n",
        "        \"\"\"\n",
        "        catNms = catNms if _isArrayLike(catNms) else [catNms]\n",
        "        # supNms = supNms if _isArrayLike(supNms) else [supNms]\n",
        "        catIds = catIds if _isArrayLike(catIds) else [catIds]\n",
        "\n",
        "        if len(catNms) == len(catIds) == 0:\n",
        "            cats = self.dataset['categories']\n",
        "        else:\n",
        "            cats = self.dataset['categories']\n",
        "            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat['name']          in catNms]\n",
        "            # cats = cats if len(supNms) == 0 else [cat for cat in cats if cat['supercategory'] in supNms]\n",
        "            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat['id']            in catIds]\n",
        "        ids = [cat['id'] for cat in cats]\n",
        "        return ids\n",
        "\n",
        "    def getImgIds(self, imgIds=[], catIds=[]):\n",
        "        '''\n",
        "        Get img ids that satisfy given filter conditions.\n",
        "        :param imgIds (int array) : get imgs for given ids\n",
        "        :param catIds (int array) : get imgs with all given cats\n",
        "        :return: ids (int array)  : integer array of img ids\n",
        "        '''\n",
        "        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n",
        "        catIds = catIds if _isArrayLike(catIds) else [catIds]\n",
        "\n",
        "        if len(imgIds) == len(catIds) == 0:\n",
        "            ids = self.imgs.keys()\n",
        "        else:\n",
        "            ids = set(imgIds)\n",
        "            for i, catId in enumerate(catIds):\n",
        "                if i == 0 and len(ids) == 0:\n",
        "                    ids = set(self.catToImgs[catId])\n",
        "                else:\n",
        "                    ids &= set(self.catToImgs[catId])\n",
        "        return list(ids)\n",
        "\n",
        "    def cls(self, ids=[]):\n",
        "        target = [0] * 80\n",
        "        a = ([self.anns[id]['category_id'] for id in ids])\n",
        "        target[a[0] - 1] = 1\n",
        "        return target\n",
        "    \n",
        "    def loadAnns(self, ids=[]):\n",
        "        \"\"\"\n",
        "        Load anns with the specified ids.\n",
        "        :param ids (int array)       : integer ids specifying anns\n",
        "        :return: anns (object array) : loaded ann objects\n",
        "        \"\"\"\n",
        "        if _isArrayLike(ids):\n",
        "            # list_of_targets = [self.cls(ids)]\n",
        "            # np_of_targets = np.asarray(list_of_targets)\n",
        "            # return np_of_targets\n",
        "            # return [self.cls(ids)]\n",
        "            return self.cls(ids)\n",
        "            # return print(type([self.anns[id] for id in ids]))\n",
        "        elif type(ids) == int:\n",
        "            # print('int')\n",
        "            return [self.anns[ids]]\n",
        "\n",
        "    def loadCats(self, ids=[]):\n",
        "        \"\"\"\n",
        "        Load cats with the specified ids.\n",
        "        :param ids (int array)       : integer ids specifying cats\n",
        "        :return: cats (object array) : loaded cat objects\n",
        "        \"\"\"\n",
        "        if _isArrayLike(ids):\n",
        "            return [self.cats[id] for id in ids]\n",
        "        elif type(ids) == int:\n",
        "            return [self.cats[ids]]\n",
        "\n",
        "    def loadImgs(self, ids=[]):\n",
        "        \"\"\"\n",
        "        Load anns with the specified ids.\n",
        "        :param ids (int array)       : integer ids specifying img\n",
        "        :return: imgs (object array) : loaded img objects\n",
        "        \"\"\"\n",
        "        if _isArrayLike(ids):\n",
        "            return [self.imgs[id] for id in ids]\n",
        "        elif type(ids) == int:\n",
        "            return [self.imgs[ids]]\n",
        "\n",
        "    def download(self, tarDir = None, imgIds = [] ):\n",
        "        '''\n",
        "        Download COCO images from mscoco.org server.\n",
        "        :param tarDir (str): COCO results directory name\n",
        "               imgIds (list): images to be downloaded\n",
        "        :return:\n",
        "        '''\n",
        "        if tarDir is None:\n",
        "            print('Please specify target directory')\n",
        "            return -1\n",
        "        if len(imgIds) == 0:\n",
        "            imgs = self.imgs.values()\n",
        "        else:\n",
        "            imgs = self.loadImgs(imgIds)\n",
        "        N = len(imgs)\n",
        "        if not os.path.exists(tarDir):\n",
        "            os.makedirs(tarDir)\n",
        "        for i, img in enumerate(imgs):\n",
        "            tic = time.time()\n",
        "            fname = os.path.join(tarDir, img['file_name'])\n",
        "            if not os.path.exists(fname):\n",
        "                urlretrieve(img['coco_url'], fname)\n",
        "            print('downloaded {}/{} images (t={:0.1f}s)'.format(i, N, time.time()- tic))\n",
        "\n",
        "    def loadNumpyAnnotations(self, data):\n",
        "        \"\"\"\n",
        "        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n",
        "        :param  data (numpy.ndarray)\n",
        "        :return: annotations (python nested list)\n",
        "        \"\"\"\n",
        "        print('Converting ndarray to lists...')\n",
        "        assert(type(data) == np.ndarray)\n",
        "        print(data.shape)\n",
        "        assert(data.shape[1] == 7)\n",
        "        N = data.shape[0]\n",
        "        ann = []\n",
        "        for i in range(N):\n",
        "            if i % 1000000 == 0:\n",
        "                print('{}/{}'.format(i,N))\n",
        "            ann += [{\n",
        "                'image_id'  : int(data[i, 0]),\n",
        "                'bbox'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n",
        "                'score' : data[i, 5],\n",
        "                'category_id': int(data[i, 6]),\n",
        "                }]\n",
        "        return ann\n",
        "\n",
        "    def annToRLE(self, ann):\n",
        "        \"\"\"\n",
        "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
        "        :return: binary mask (numpy 2D array)\n",
        "        \"\"\"\n",
        "        t = self.imgs[ann['image_id']]\n",
        "        h, w = t['height'], t['width']\n",
        "        segm = ann['segmentation']\n",
        "        if type(segm) == list:\n",
        "            # polygon -- a single object might consist of multiple parts\n",
        "            # we merge all parts into one mask rle code\n",
        "            rles = maskUtils.frPyObjects(segm, h, w)\n",
        "            rle = maskUtils.merge(rles)\n",
        "        elif type(segm['counts']) == list:\n",
        "            # uncompressed RLE\n",
        "            rle = maskUtils.frPyObjects(segm, h, w)\n",
        "        else:\n",
        "            # rle\n",
        "            rle = ann['segmentation']\n",
        "        return rle\n",
        "\n",
        "    def annToMask(self, ann):\n",
        "        \"\"\"\n",
        "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
        "        :return: binary mask (numpy 2D array)\n",
        "        \"\"\"\n",
        "        rle = self.annToRLE(ann)\n",
        "        m = maskUtils.decode(rle)\n",
        "        return m\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class my_CocoDetection(VisionDataset):\n",
        "    \"\"\"`MS Coco Detection <https://cocodataset.org/#detection-2016>`_ Dataset.\n",
        "\n",
        "    It requires the `COCO API to be installed <https://github.com/pdollar/coco/tree/master/PythonAPI>`_.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory where images are downloaded to.\n",
        "        annFile (string): Path to json annotation file.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
        "            and returns a transformed version.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        annFile: str,\n",
        "        transform: Optional[Callable] = None,\n",
        "        target_transform: Optional[Callable] = None,\n",
        "        transforms: Optional[Callable] = None,\n",
        "    ) -> None:\n",
        "        super().__init__(root, transforms, transform, target_transform)\n",
        "        from pycocotools.coco import COCO\n",
        "\n",
        "        self.coco = my_COCO(annFile)\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "\n",
        "    def _load_image(self, id: int) -> Image.Image:\n",
        "        path = self.coco.loadImgs(id)[0][\"file_name\"]\n",
        "        # im = Image.open(os.path.join(self.root, os.path.basename(path))).convert(\"RGB\")\n",
        "        # display(im)\n",
        "        return Image.open(os.path.join(self.root, os.path.basename(path)))\n",
        "\n",
        "    def _load_target(self, id: int):\n",
        "        # target = self.coco.loadAnns(self.coco.getAnnIds(id))\n",
        "        # if self.target_transform is not None:\n",
        "        #   target = self.target_transform(target)\n",
        "        # return target\n",
        "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "        id = self.ids[index]\n",
        "        image = self._load_image(id)\n",
        "        # target = self._load_target(id)\n",
        "        target = torch.as_tensor(self._load_target(id), dtype = torch.float32)\n",
        "        # print(type(image))\n",
        "        if self.transforms is not None:\n",
        "            image, target = self.transforms(image, target)\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.ids)\n",
        "    \n",
        "    def printf(self):\n",
        "      print(self.root)\n"
      ],
      "metadata": {
        "id": "-8hzSbkV2-2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = Compose([PILToTensor(), ConvertImageDtype(torch.float)])"
      ],
      "metadata": {
        "id": "gmo4Y7ulsru7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path2data=\"/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_val/new_data\"\n",
        "path2json=\"/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_val/ann.json\""
      ],
      "metadata": {
        "id": "Vbvi8VRPQJzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco = my_CocoDetection(path2data, path2json, transform = transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkVl4Br8QhvX",
        "outputId": "05f51943-39b0-4348-af8d-cbae24294c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.04s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coco"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNFIVOO02MOS",
        "outputId": "2c66cdc9-a104-45e4-dfbc-0be6e37904cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset my_CocoDetection\n",
              "    Number of datapoints: 15974\n",
              "    Root location: /content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_val/new_data\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               PILToTensor()\n",
              "               ConvertImageDtype()\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coco.__getitem__(58)"
      ],
      "metadata": {
        "id": "3a85CLFvrdjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco._load_target(12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9moHjDOOwFB",
        "outputId": "389a7c7d-fa85-4f28-ff5d-40967b4ea26c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64"
      ],
      "metadata": {
        "id": "fodFyDoUvMib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco_loader = DataLoader(coco, batch_size, shuffle=True, num_workers = 0)"
      ],
      "metadata": {
        "id": "NgOlWxuUvI3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "metadata": {
        "id": "Sl9rFccegmBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as dset"
      ],
      "metadata": {
        "id": "3iyiIZQHg7bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_default_device()\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro902skDh3Ru",
        "outputId": "d8243d2a-be50-445f-f314-d5a46129f007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(outputs, labels):\n",
        "    # _, preds = torch.max(outputs, dim=1)\n",
        "    # return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    _, targs = torch.max(labels, dim=1)\n",
        "    # print(preds)\n",
        "    # print(targs)\n",
        "    return torch.tensor(torch.sum(preds == targs).item() / len(preds))\n",
        "\n",
        "\n",
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    # rewrite\n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        labels1 = labels.softmax(dim=1)\n",
        "        # print(labels, labels.size())\n",
        "        # print(labels)\n",
        "        # print(out)\n",
        "        # print (out, out.size())\n",
        "        loss = F.cross_entropy(out, labels1)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "      \n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
      ],
      "metadata": {
        "id": "2OJmfBUlh5ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet34 = models.resnet34()\n",
        "resnet34"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvcheV5yh6tu",
        "outputId": "613f54b7-10dd-49c8-bfdc-79c663251d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FLIR80Resnet(ImageClassificationBase):\n",
        "        def __init__(self):\n",
        "                super().__init__()\n",
        "                self.network = torchvision.models.resnet34()\n",
        "                num_ftrs = self.network.fc.in_features\n",
        "                self.network.fc = nn.Linear(num_ftrs, 80)\n",
        "        \n",
        "        def forward(self, xb):\n",
        "                return self.network(xb)\n",
        "            \n",
        "model = FLIR80Resnet()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o8Pcu1fh9tS",
        "outputId": "0296d304-0a26-49ba-bd7a-92108e21f609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FLIR80Resnet(\n",
              "  (network): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (5): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(in_features=512, out_features=80, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = to_device(FLIR80Resnet(), device)"
      ],
      "metadata": {
        "id": "bHXW8F4DiB-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    # outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    outputs2 = []\n",
        "    i = 0\n",
        "    for batch in val_loader:\n",
        "      print(i)\n",
        "      i+=1\n",
        "      outputs2.append(model.validation_step(batch))\n",
        "    return model.validation_epoch_end(outputs2)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, \n",
        "        weight_decay=0, opt_func=torch.optim.SGD):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr, weight_decay=weight_decay)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ],
      "metadata": {
        "id": "MXlr4vGAiGSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = [evaluate(model, coco_loader)]\n",
        "history"
      ],
      "metadata": {
        "id": "yuQp9Ht2iI4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49da045f-2336-46e0-c04a-7462088bf1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'val_acc': 0.0, 'val_loss': 26.603683471679688}]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qMYYT_PWEyhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.randn(3, 5).softmax(dim=1)\n",
        "print(input)\n",
        "print(target)\n",
        "loss = F.cross_entropy(input, target)\n",
        "loss.backward()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6owSf_U2upM",
        "outputId": "40e984e8-f5a1-4b67-8048-5e8d625dc0c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.6092,  1.5785,  0.3189,  0.5635, -0.0618],\n",
            "        [-1.0746, -1.8930,  1.2565, -0.7404, -0.8455],\n",
            "        [-0.9029, -0.0176,  0.2300, -0.2433, -0.1535]], requires_grad=True)\n",
            "tensor([[0.0612, 0.2935, 0.2335, 0.0501, 0.3618],\n",
            "        [0.0497, 0.0884, 0.6734, 0.0731, 0.1155],\n",
            "        [0.4664, 0.2580, 0.0399, 0.0979, 0.1377]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path2data=\"/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_val/data\"\n",
        "path2json=\"/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_val/coco.json\""
      ],
      "metadata": {
        "id": "lBbwqetTX36N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco_train_detect = my_CocoDetection(path2data, path2json, transform=PILToTensor())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaX3LV9X3DZX",
        "outputId": "3587a044-cb5a-4e0b-e568-e10ef7a37a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=2.26s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# return all targets from image_id (arguments = 'image_id')\n",
        "coco_train_detect.__getitem__(1)"
      ],
      "metadata": {
        "id": "MadU3nN16O9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco_train_detect._load_target(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ONuRX3TAWOF",
        "outputId": "7269932c-5d96-4b7a-e801-db180a6fc1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'area': 110,\n",
              "  'bbox': [360, 217, 10, 11],\n",
              "  'category_id': 12,\n",
              "  'extra_info': {'human_annotated': 'human', 'occluded': 'no_(fully_visible)'},\n",
              "  'id': 16,\n",
              "  'image_id': 1,\n",
              "  'iscrowd': False,\n",
              "  'segmentation': [[360, 217, 370, 217, 360, 228, 370, 228]]},\n",
              " {'area': 189,\n",
              "  'bbox': [199, 223, 21, 9],\n",
              "  'category_id': 12,\n",
              "  'extra_info': {'human_annotated': 'human',\n",
              "   'occluded': '1%_-_70%_occluded_(partially_occluded)'},\n",
              "  'id': 17,\n",
              "  'image_id': 1,\n",
              "  'iscrowd': False,\n",
              "  'segmentation': [[199, 223, 220, 223, 199, 232, 220, 232]]},\n",
              " {'area': 602,\n",
              "  'bbox': [489, 222, 14, 43],\n",
              "  'category_id': 1,\n",
              "  'extra_info': {'human_annotated': 'human',\n",
              "   'occluded': 'no_(fully_visible)',\n",
              "   'specify_if_human_or_not': 'human'},\n",
              "  'id': 18,\n",
              "  'image_id': 1,\n",
              "  'iscrowd': False,\n",
              "  'segmentation': [[489, 222, 503, 222, 489, 265, 503, 265]]},\n",
              " {'area': 748,\n",
              "  'bbox': [421, 245, 34, 22],\n",
              "  'category_id': 2,\n",
              "  'extra_info': {'human_annotated': 'human',\n",
              "   'occluded': '1%_-_70%_occluded_(partially_occluded)'},\n",
              "  'id': 19,\n",
              "  'image_id': 1,\n",
              "  'iscrowd': False,\n",
              "  'segmentation': [[421, 245, 455, 245, 421, 267, 455, 267]]},\n",
              " {'area': 8449,\n",
              "  'bbox': [75, 228, 119, 71],\n",
              "  'category_id': 3,\n",
              "  'extra_info': {'human_annotated': 'human', 'occluded': 'no_(fully_visible)'},\n",
              "  'id': 20,\n",
              "  'image_id': 1,\n",
              "  'iscrowd': False,\n",
              "  'segmentation': [[75, 228, 194, 228, 75, 299, 194, 299]]},\n",
              " {'area': 2632,\n",
              "  'bbox': [271, 224, 56, 47],\n",
              "  'category_id': 3,\n",
              "  'extra_info': {'human_annotated': 'human', 'occluded': 'no_(fully_visible)'},\n",
              "  'id': 21,\n",
              "  'image_id': 1,\n",
              "  'iscrowd': False,\n",
              "  'segmentation': [[271, 224, 327, 224, 271, 271, 327, 271]]},\n",
              " {'area': 475,\n",
              "  'bbox': [235, 236, 25, 19],\n",
              "  'category_id': 3,\n",
              "  'extra_info': {'human_annotated': 'human', 'occluded': 'no_(fully_visible)'},\n",
              "  'id': 22,\n",
              "  'image_id': 1,\n",
              "  'iscrowd': False,\n",
              "  'segmentation': [[235, 236, 260, 236, 235, 255, 260, 255]]}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bl9xPD8XPKeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create new catalog with objects "
      ],
      "metadata": {
        "id": "iMhmeDP04JZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def max_coord(file_name):\n",
        "    x_max = 0\n",
        "    y_max = 0\n",
        "    count_1024 = 0\n",
        "    count = 0\n",
        "    for i in file_name[\"annotations\"]:\n",
        "        count += 1\n",
        "        if i['bbox'][2] > x_max and i['bbox'][2] <= 256:\n",
        "            x_max = i['bbox'][2]\n",
        "        if i['bbox'][2] <=256 and i['bbox'][3] <= 256:\n",
        "            count_1024 += 1\n",
        "        if i['bbox'][3] > y_max and i['bbox'][3] <= 256:\n",
        "            y_max = i['bbox'][3]\n",
        "    print('(x_max:', x_max, '\\ty_max:', y_max, ')')\n",
        "    print('count_256: ', count_1024, '\\tcount', count)\n",
        "    print('-----------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXHTi0m04HSi",
        "outputId": "b2916ce5-11b2-4cba-9b96-8af59b28cce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(x_max: 256 \ty_max: 256 )\n",
            "count_256:  158550 \tcount 169174\n",
            "-----------------------------\n",
            "(x_max: 256 \ty_max: 256 )\n",
            "count_256:  15974 \tcount 16909\n",
            "-----------------------------\n",
            "(x_max: 256 \ty_max: 256 )\n",
            "count_256:  174331 \tcount 175040\n",
            "-----------------------------\n",
            "(x_max: 255 \ty_max: 256 )\n",
            "count_256:  16635 \tcount 16696\n",
            "-----------------------------\n",
            "(x_max: 256 \ty_max: 256 )\n",
            "count_256:  83661 \tcount 84786\n",
            "-----------------------------\n",
            "(x_max: 189 \ty_max: 253 )\n",
            "count_256:  62289 \tcount 62317\n",
            "-----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Old code without overriding annotation"
      ],
      "metadata": {
        "id": "fxauclQ0gspX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_img(path, file_name):\n",
        "  shift = 256\n",
        "  x_left = 0\n",
        "  y_left = 0\n",
        "  x_right = 0\n",
        "  y_right = 0\n",
        "  for i in file_name[\"annotations\"]:\n",
        "    if (i['bbox'][2] <= shift and i['bbox'][3] <= shift):\n",
        "      if i['bbox'][2] % 2 == 1:\n",
        "        i['bbox'][2] += 1\n",
        "      if i['bbox'][3] % 2 == 1:\n",
        "        i['bbox'][3] += 1\n",
        "      a = int((shift - i['bbox'][2])/2)\n",
        "      b = int((shift - i['bbox'][3])/2)\n",
        "      x_left = i['bbox'][0] - a\n",
        "      y_left = i['bbox'][1] - b\n",
        "      x_right = i['bbox'][0] + i['bbox'][2] + a\n",
        "      y_right = i['bbox'][1] + i['bbox'][3] + b\n",
        "      for j in file_name[\"images\"]:\n",
        "        if i['image_id'] == j['id']:\n",
        "          img = Image.open(path + j['file_name'])\n",
        "          area = (x_left, y_left, x_right, y_right)\n",
        "          cropped_img = img.crop(area)\n",
        "          cropped_img.save(str(path + 'new_data/' + (os.path.splitext(os.path.basename(j['file_name'])))[0] + '_obj' + str(i['id']) + '.jpg'))\n",
        "          break"
      ],
      "metadata": {
        "id": "o-6157Nb807J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "New code with overriding annotation"
      ],
      "metadata": {
        "id": "oND9uB_bg0SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def write_json(new_data, new_data2, new_data3, new_data4, new_data5, new_data6,  filename = 'ann.json'):\n",
        "def write_json_ann(new_data, new_data2, new_data3, new_data4, new_data5, new_data6,  path):\n",
        "    filename = path + 'ann.json'\n",
        "    with open(filename, 'r+') as file:\n",
        "        # First we load existing data into a dict.\n",
        "        file_data = json.load(file)\n",
        "        # Join new_data with file_data inside annotation\n",
        "        new = {new_data: new_data2, new_data3: new_data4, new_data5: new_data6}\n",
        "        file_data[\"annotations\"].append(new)\n",
        "        # Sets file's current position at offset.\n",
        "        file.seek(0)\n",
        "        # convert back to json.\n",
        "        json.dump(file_data, file, indent = 4)"
      ],
      "metadata": {
        "id": "YEDuS-NIfFKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def write_json(new_data, new_data2, new_data3, new_data4, filename = 'ann.json'):\n",
        "def write_json_cat(new_data, new_data2, new_data3, new_data4, path):\n",
        "    filename = path + 'ann.json'\n",
        "    with open(filename, 'r+') as file:\n",
        "        file_data = json.load(file)\n",
        "        new = {new_data: new_data2, new_data3: new_data4}\n",
        "        file_data[\"categories\"].append(new)\n",
        "        file.seek(0)\n",
        "        json.dump(file_data, file, indent = 4)"
      ],
      "metadata": {
        "id": "zd-3oJOofcOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_img_with_ann(path, file_name):\n",
        "  shift = 256\n",
        "  x_left = 0\n",
        "  y_left = 0\n",
        "  x_right = 0\n",
        "  y_right = 0\n",
        "  for i in file_name[\"annotations\"]:\n",
        "    if (i['bbox'][2] <= 256 and i['bbox'][3] <= 256):\n",
        "      if i['bbox'][2] % 2 == 1:\n",
        "        i['bbox'][2] += 1\n",
        "      if i['bbox'][3] % 2 == 1:\n",
        "        i['bbox'][3] += 1\n",
        "      a = int((shift - i['bbox'][2])/2)\n",
        "      b = int((shift - i['bbox'][3])/2)\n",
        "      x_left = i['bbox'][0] - a\n",
        "      y_left = i['bbox'][1] - b\n",
        "      x_right = i['bbox'][0] + i['bbox'][2] + a\n",
        "      y_right = i['bbox'][1] + i['bbox'][3] + b\n",
        "      for j in file_name[\"images\"]:\n",
        "        if i['image_id'] == j['id']:\n",
        "          img = Image.open(path + j['file_name'])\n",
        "          area = (x_left, y_left, x_right, y_right)\n",
        "          cropped_img = img.crop(area)\n",
        "          cropped_img.save(str(path + 'new_data/' + (os.path.splitext(os.path.basename(j['file_name'])))[0] + '_obj' + str(i['id']) + '.jpg'))\n",
        "          write_json_ann(\"id\", i['id'], \"category_id\", i['category_id'], \"file_name\", str(path + 'new_data/' + (os.path.splitext(os.path.basename(j['file_name'])))[0] + '_obj' + str(i['id']) + '.jpg'), path)\n",
        "          break\n",
        "  for i in file_name[\"categories\"]:\n",
        "    write_json_cat(\"id\", i['id'], \"name\", i['name'], path)"
      ],
      "metadata": {
        "id": "IQVrbmCkg4qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_train/'\n",
        "f1 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_train/coco.json')\n",
        "f2 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_val/coco.json')\n",
        "f3 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_thermal_train/coco.json')\n",
        "f4 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_thermal_val/coco.json')\n",
        "f5 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/video_rgb_test/coco.json')\n",
        "f6 = open('/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/video_thermal_test/coco.json')\n",
        "\n",
        "data1 = json.load(f1)\n",
        "data2 = json.load(f2)\n",
        "data3 = json.load(f3)\n",
        "data4 = json.load(f4)\n",
        "data5 = json.load(f5)\n",
        "data6 = json.load(f6)\n",
        "\n",
        "max_coord(data1)\n",
        "max_coord(data2)\n",
        "max_coord(data3)\n",
        "max_coord(data4)\n",
        "max_coord(data5)\n",
        "max_coord(data6)\n",
        "\n",
        "f1.close()\n",
        "f2.close()\n",
        "f3.close()\n",
        "f4.close()\n",
        "f5.close()\n",
        "f6.close()"
      ],
      "metadata": {
        "id": "lEwsXe2S8sKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_rgb_train/'\n",
        "f1 = open(path + 'coco.json')\n",
        "data1 = json.load(f1)\n",
        "crop_img_with_ann(path, data1)\n",
        "f1.close()"
      ],
      "metadata": {
        "id": "38dqJraXgZyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вот эту запусти"
      ],
      "metadata": {
        "id": "xRtFATI3ke45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_thermal_val/'\n",
        "f1 = open(path + 'coco.json')\n",
        "data1 = json.load(f1)\n",
        "crop_img_with_ann(path, data1)\n",
        "f1.close()"
      ],
      "metadata": {
        "id": "5Q2kpGQNpn-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/images_thermal_train/'\n",
        "f1 = open(path + 'coco.json')\n",
        "data1 = json.load(f1)\n",
        "crop_img_with_ann(path, data1)\n",
        "f1.close()"
      ],
      "metadata": {
        "id": "VMx0AAh5pt2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/video_rgb_test/'\n",
        "f1 = open(path + 'coco.json')\n",
        "data1 = json.load(f1)\n",
        "crop_img_with_ann(path, data1)\n",
        "f1.close()"
      ],
      "metadata": {
        "id": "Avn08cRqpzon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/FLIR_ADAS_v2/FLIR_ADAS_v2/video_thermal_test/'\n",
        "f1 = open(path + 'coco.json')\n",
        "data1 = json.load(f1)\n",
        "crop_img_with_ann(path, data1)\n",
        "f1.close()"
      ],
      "metadata": {
        "id": "YPG4PxK8qsX3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "lQUBXJjztVC9",
        "i3sjpkPotiiz"
      ],
      "name": "crop_picture",
      "provenance": [],
      "mount_file_id": "11B5Ovru1G3AkRLh4A-6OOGpjgGlZ6Gi2",
      "authorship_tag": "ABX9TyP/6MyZLW50I9PbOxVOuZe7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}